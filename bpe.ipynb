{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19917b92-d880-40f5-9b3d-7351721af811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def byte_level_tokenize(text):\n",
    "    byte_tokens = list(text.encode(\"utf-8\")) \n",
    "    return byte_tokens\n",
    "\n",
    "def byte_level_detokenize(byte_tokens):\n",
    "    text = bytes(byte_tokens).decode(\"utf-8\")  \n",
    "    return text\n",
    "\n",
    "def get_byte_vocab(tokens):\n",
    "    vocab = Counter()\n",
    "    for pair in zip(tokens[:-1], tokens[1:]):\n",
    "        vocab[pair] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_most_frequent_pair(vocab):\n",
    "    most_common = vocab.most_common()\n",
    "    if len(most_common) > 0:\n",
    "        return most_common[0][0]\n",
    "    return None\n",
    "\n",
    "def merge_pairs(tokens, pair, next_id):\n",
    "    updated_tokens = []\n",
    "    a, b = pair\n",
    "    idx = 0\n",
    "    while idx < len(tokens):\n",
    "        if idx < len(tokens) - 1 and tokens[idx] == a and tokens[idx + 1] == b:\n",
    "            updated_tokens.append(next_id)\n",
    "            idx += 2 \n",
    "        else:\n",
    "            updated_tokens.append(tokens[idx])\n",
    "            idx += 1\n",
    "\n",
    "    return updated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf5670ad-9e44-4091-bdf2-b58a21c102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, vocab_size, dest_folder):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokens = []\n",
    "        self.merges = []\n",
    "        self.token_to_char = {i: bytes([i]) for i in range(256)}\n",
    "        self.corpus = \"\"\n",
    "        self.dest_folder = dest_folder\n",
    "\n",
    "    def __call__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.tokens, self.merges =  self.byte_pair()\n",
    "        self.save()\n",
    "        \n",
    "    def byte_pair(self):\n",
    "        tokens = byte_level_tokenize(self.corpus)\n",
    "        print(f\"Current tokens are {tokens}\")\n",
    "        print(f\"Length of the current tokens are {len(tokens)}\")\n",
    "        tokens_copy = tokens.copy()\n",
    "        next_id = 256\n",
    "        merges = []\n",
    "        for idx in range(self.vocab_size-255):\n",
    "            vocab = get_byte_vocab(tokens)\n",
    "            pair = get_most_frequent_pair(vocab)\n",
    "            if pair is None:\n",
    "                break\n",
    "            self.token_to_char[next_id] = self.token_to_char[pair[0]]+self.token_to_char[pair[1]]\n",
    "            merges.append((pair, next_id))\n",
    "            print(f\"Most frequent pair is {pair}\")\n",
    "            tokens = merge_pairs(tokens, pair, next_id)\n",
    "            next_id += 1\n",
    "            print(f\"Next id is {next_id}\")\n",
    "        print(f\"Final Tokens are {tokens}.\")\n",
    "        print(f\"Length of the final tokens are {len(tokens)}\")\n",
    "        return tokens, merges\n",
    "\n",
    "    def save(self):\n",
    "        model_data = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'tokens': self.tokens,\n",
    "            'merges': self.merges,\n",
    "            'token_to_char': self.token_to_char\n",
    "        }\n",
    "        filepath = os.path.join(self.dest_folder, \"bpe.pk1\")\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"BPE model successfully saved to {filepath}\")\n",
    "\n",
    "    def encode(self, corpus):\n",
    "        tokens = byte_level_tokenize(corpus)\n",
    "        for merge in self.merges:\n",
    "            tokens = merge_pairs(tokens, merge[0], merge[1])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \"\".join(\n",
    "            self.token_to_char[token].decode(\"utf-8\") \n",
    "            if isinstance(self.token_to_char[token], bytes) \n",
    "            else self.token_to_char[token]  \n",
    "            for token in tokens\n",
    "        )\n",
    "\n",
    "    def save(self):\n",
    "        model_data = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'tokens': self.tokens,\n",
    "            'merges': self.merges,\n",
    "            'token_to_char': self.token_to_char\n",
    "        }\n",
    "        filepath = os.path.join(self.dest_folder, \"bpe.pk1\")\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"BPE model successfully saved to {filepath}\")\n",
    "\n",
    "class BPETokenizer:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        tokenizer = BPE(data['vocab_size'], data['merges'])\n",
    "        tokenizer.tokens = data['tokens']\n",
    "        tokenizer.token_to_char = data['token_to_char']\n",
    "        return tokenizer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27cd8ebe-a0b4-4448-bb2c-568725ce1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''To deal with this unknown word problem, modern tokenizers automatically in-\n",
    "duce sets of tokens that include tokens smaller than words, called subwords. Sub-subwords\n",
    "words can be arbitrary substrings, or they can be meaning-bearing units like the\n",
    "morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit of a lan-\n",
    "guage; for example the word unwashable has the morphemes un-, wash, and -able.)\n",
    "In modern tokenization schemes, most tokens are words, but some tokens are fre-\n",
    "quently occurring morphemes or other subwords like -er. Every unseen word like\n",
    "lower can thus be represented by some sequence of known subword units, such as\n",
    "low and er, or even as a sequence of individual letters if necessary'''\n",
    "#corpus = '''To deal with this'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "665437ee-8e52-4226-adbd-1e3a955d7f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tokens are [84, 111, 32, 100, 101, 97, 108, 32, 119, 105, 116, 104, 32, 116, 104, 105, 115, 32, 117, 110, 107, 110, 111, 119, 110, 32, 119, 111, 114, 100, 32, 112, 114, 111, 98, 108, 101, 109, 44, 32, 109, 111, 100, 101, 114, 110, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 115, 32, 97, 117, 116, 111, 109, 97, 116, 105, 99, 97, 108, 108, 121, 32, 105, 110, 45, 10, 100, 117, 99, 101, 32, 115, 101, 116, 115, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 32, 116, 104, 97, 116, 32, 105, 110, 99, 108, 117, 100, 101, 32, 116, 111, 107, 101, 110, 115, 32, 115, 109, 97, 108, 108, 101, 114, 32, 116, 104, 97, 110, 32, 119, 111, 114, 100, 115, 44, 32, 99, 97, 108, 108, 101, 100, 32, 115, 117, 98, 119, 111, 114, 100, 115, 46, 32, 83, 117, 98, 45, 115, 117, 98, 119, 111, 114, 100, 115, 10, 119, 111, 114, 100, 115, 32, 99, 97, 110, 32, 98, 101, 32, 97, 114, 98, 105, 116, 114, 97, 114, 121, 32, 115, 117, 98, 115, 116, 114, 105, 110, 103, 115, 44, 32, 111, 114, 32, 116, 104, 101, 121, 32, 99, 97, 110, 32, 98, 101, 32, 109, 101, 97, 110, 105, 110, 103, 45, 98, 101, 97, 114, 105, 110, 103, 32, 117, 110, 105, 116, 115, 32, 108, 105, 107, 101, 32, 116, 104, 101, 10, 109, 111, 114, 112, 104, 101, 109, 101, 115, 32, 45, 101, 115, 116, 32, 111, 114, 32, 45, 101, 114, 46, 32, 40, 65, 32, 109, 111, 114, 112, 104, 101, 109, 101, 32, 105, 115, 32, 116, 104, 101, 32, 115, 109, 97, 108, 108, 101, 115, 116, 32, 109, 101, 97, 110, 105, 110, 103, 45, 98, 101, 97, 114, 105, 110, 103, 32, 117, 110, 105, 116, 32, 111, 102, 32, 97, 32, 108, 97, 110, 45, 10, 103, 117, 97, 103, 101, 59, 32, 102, 111, 114, 32, 101, 120, 97, 109, 112, 108, 101, 32, 116, 104, 101, 32, 119, 111, 114, 100, 32, 117, 110, 119, 97, 115, 104, 97, 98, 108, 101, 32, 104, 97, 115, 32, 116, 104, 101, 32, 109, 111, 114, 112, 104, 101, 109, 101, 115, 32, 117, 110, 45, 44, 32, 119, 97, 115, 104, 44, 32, 97, 110, 100, 32, 45, 97, 98, 108, 101, 46, 41, 10, 73, 110, 32, 109, 111, 100, 101, 114, 110, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 115, 99, 104, 101, 109, 101, 115, 44, 32, 109, 111, 115, 116, 32, 116, 111, 107, 101, 110, 115, 32, 97, 114, 101, 32, 119, 111, 114, 100, 115, 44, 32, 98, 117, 116, 32, 115, 111, 109, 101, 32, 116, 111, 107, 101, 110, 115, 32, 97, 114, 101, 32, 102, 114, 101, 45, 10, 113, 117, 101, 110, 116, 108, 121, 32, 111, 99, 99, 117, 114, 114, 105, 110, 103, 32, 109, 111, 114, 112, 104, 101, 109, 101, 115, 32, 111, 114, 32, 111, 116, 104, 101, 114, 32, 115, 117, 98, 119, 111, 114, 100, 115, 32, 108, 105, 107, 101, 32, 45, 101, 114, 46, 32, 69, 118, 101, 114, 121, 32, 117, 110, 115, 101, 101, 110, 32, 119, 111, 114, 100, 32, 108, 105, 107, 101, 10, 108, 111, 119, 101, 114, 32, 99, 97, 110, 32, 116, 104, 117, 115, 32, 98, 101, 32, 114, 101, 112, 114, 101, 115, 101, 110, 116, 101, 100, 32, 98, 121, 32, 115, 111, 109, 101, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 107, 110, 111, 119, 110, 32, 115, 117, 98, 119, 111, 114, 100, 32, 117, 110, 105, 116, 115, 44, 32, 115, 117, 99, 104, 32, 97, 115, 10, 108, 111, 119, 32, 97, 110, 100, 32, 101, 114, 44, 32, 111, 114, 32, 101, 118, 101, 110, 32, 97, 115, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 32, 108, 101, 116, 116, 101, 114, 115, 32, 105, 102, 32, 110, 101, 99, 101, 115, 115, 97, 114, 121]\n",
      "Length of the current tokens are 715\n",
      "Most frequent pair is (111, 114)\n",
      "Next id is 257\n",
      "Most frequent pair is (101, 32)\n",
      "Next id is 258\n",
      "Most frequent pair is (115, 32)\n",
      "Next id is 259\n",
      "Most frequent pair is (110, 32)\n",
      "Next id is 260\n",
      "Most frequent pair is (116, 104)\n",
      "Next id is 261\n",
      "Most frequent pair is (101, 114)\n",
      "Next id is 262\n",
      "Most frequent pair is (119, 256)\n",
      "Next id is 263\n",
      "Most frequent pair is (262, 100)\n",
      "Next id is 264\n",
      "Most frequent pair is (101, 110)\n",
      "Next id is 265\n",
      "Most frequent pair is (44, 32)\n",
      "Next id is 266\n",
      "Most frequent pair is (105, 110)\n",
      "Next id is 267\n",
      "Most frequent pair is (117, 110)\n",
      "Next id is 268\n",
      "Most frequent pair is (116, 111)\n",
      "Next id is 269\n",
      "Most frequent pair is (97, 114)\n",
      "Next id is 270\n",
      "Most frequent pair is (97, 108)\n",
      "Next id is 271\n",
      "Final Tokens are [84, 111, 32, 100, 101, 270, 32, 119, 105, 260, 32, 260, 105, 258, 267, 107, 110, 111, 119, 259, 263, 32, 112, 114, 111, 98, 108, 101, 109, 265, 109, 111, 100, 261, 259, 268, 107, 264, 105, 122, 261, 258, 97, 117, 268, 109, 97, 116, 105, 99, 270, 108, 121, 32, 266, 45, 10, 100, 117, 99, 257, 115, 101, 116, 258, 111, 102, 32, 268, 107, 264, 258, 260, 97, 116, 32, 266, 99, 108, 117, 100, 257, 268, 107, 264, 258, 115, 109, 270, 108, 261, 32, 260, 97, 259, 263, 115, 265, 99, 270, 108, 101, 100, 32, 115, 117, 98, 263, 115, 46, 32, 83, 117, 98, 45, 115, 117, 98, 263, 115, 10, 263, 258, 99, 97, 259, 98, 257, 269, 98, 105, 116, 114, 269, 121, 32, 115, 117, 98, 115, 116, 114, 266, 103, 115, 265, 256, 32, 260, 101, 121, 32, 99, 97, 259, 98, 257, 109, 101, 97, 110, 266, 103, 45, 98, 101, 269, 266, 103, 32, 267, 105, 116, 258, 108, 105, 107, 257, 260, 101, 10, 109, 256, 112, 104, 101, 109, 101, 258, 45, 101, 115, 116, 32, 256, 32, 45, 261, 46, 32, 40, 65, 32, 109, 256, 112, 104, 101, 109, 257, 105, 258, 260, 257, 115, 109, 270, 108, 101, 115, 116, 32, 109, 101, 97, 110, 266, 103, 45, 98, 101, 269, 266, 103, 32, 267, 105, 116, 32, 111, 102, 32, 97, 32, 108, 97, 110, 45, 10, 103, 117, 97, 103, 101, 59, 32, 102, 256, 32, 101, 120, 97, 109, 112, 108, 257, 260, 257, 263, 32, 267, 119, 97, 115, 104, 97, 98, 108, 257, 104, 97, 258, 260, 257, 109, 256, 112, 104, 101, 109, 101, 258, 267, 45, 265, 119, 97, 115, 104, 265, 97, 110, 100, 32, 45, 97, 98, 108, 101, 46, 41, 10, 73, 259, 109, 111, 100, 261, 259, 268, 107, 264, 105, 122, 97, 116, 105, 111, 259, 115, 99, 104, 101, 109, 101, 115, 265, 109, 111, 115, 116, 32, 268, 107, 264, 258, 269, 257, 263, 115, 265, 98, 117, 116, 32, 115, 111, 109, 257, 268, 107, 264, 258, 269, 257, 102, 114, 101, 45, 10, 113, 117, 264, 116, 108, 121, 32, 111, 99, 99, 117, 114, 114, 266, 103, 32, 109, 256, 112, 104, 101, 109, 101, 258, 256, 32, 111, 260, 261, 32, 115, 117, 98, 263, 258, 108, 105, 107, 257, 45, 261, 46, 32, 69, 118, 261, 121, 32, 267, 115, 101, 101, 259, 263, 32, 108, 105, 107, 101, 10, 108, 111, 119, 261, 32, 99, 97, 259, 260, 117, 258, 98, 257, 114, 101, 112, 114, 101, 115, 264, 116, 101, 100, 32, 98, 121, 32, 115, 111, 109, 257, 115, 101, 113, 117, 264, 99, 257, 111, 102, 32, 107, 110, 111, 119, 259, 115, 117, 98, 263, 32, 267, 105, 116, 115, 265, 115, 117, 99, 104, 32, 97, 115, 10, 108, 111, 119, 32, 97, 110, 100, 32, 261, 265, 256, 32, 101, 118, 101, 259, 97, 258, 97, 32, 115, 101, 113, 117, 264, 99, 257, 111, 102, 32, 266, 100, 105, 118, 105, 100, 117, 270, 32, 108, 101, 116, 116, 261, 258, 105, 102, 32, 110, 101, 99, 101, 115, 115, 269, 121].\n",
      "Length of the final tokens are 550\n",
      "BPE model successfully saved to D:\\ML_And_DeepLearning\\ML_And_DeepLearning\\Implementing Byte Pair Encoding Algorithm\\bpe.pk1\n"
     ]
    }
   ],
   "source": [
    "bpe = BPE(270, r\"D:\\ML_And_DeepLearning\\ML_And_DeepLearning\\Implementing Byte Pair Encoding Algorithm\")\n",
    "bpe(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6026e13d-ce0b-45c7-84fc-f7aaf1cd65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPETokenizer.load(r\"D:\\ML_And_DeepLearning\\ML_And_DeepLearning\\Implementing Byte Pair Encoding Algorithm\\bpe.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d218b826-e67b-483d-92ca-bb8978bf1e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = model.encode(\"Modern words such as tokenization are not necessary, What happens if the words aren't in corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7157bb9-b69f-4b7c-899d-6c24fdf65b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77,\n",
       " 111,\n",
       " 100,\n",
       " 101,\n",
       " 114,\n",
       " 110,\n",
       " 32,\n",
       " 119,\n",
       " 111,\n",
       " 114,\n",
       " 100,\n",
       " 115,\n",
       " 32,\n",
       " 115,\n",
       " 117,\n",
       " 99,\n",
       " 104,\n",
       " 32,\n",
       " 97,\n",
       " 115,\n",
       " 32,\n",
       " 116,\n",
       " 111,\n",
       " 107,\n",
       " 101,\n",
       " 110,\n",
       " 105,\n",
       " 122,\n",
       " 97,\n",
       " 116,\n",
       " 105,\n",
       " 111,\n",
       " 110,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 110,\n",
       " 111,\n",
       " 116,\n",
       " 32,\n",
       " 110,\n",
       " 101,\n",
       " 99,\n",
       " 101,\n",
       " 115,\n",
       " 115,\n",
       " 97,\n",
       " 114,\n",
       " 121,\n",
       " 44,\n",
       " 32,\n",
       " 87,\n",
       " 104,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 104,\n",
       " 97,\n",
       " 112,\n",
       " 112,\n",
       " 101,\n",
       " 110,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 102,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 119,\n",
       " 111,\n",
       " 114,\n",
       " 100,\n",
       " 115,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 110,\n",
       " 39,\n",
       " 116,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 99,\n",
       " 111,\n",
       " 114,\n",
       " 112,\n",
       " 117,\n",
       " 115]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a1e27-d4fa-4945-9e0d-b8d819c2f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309c82c-8a7d-4f71-a011-812d5d8a703c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32094d6-f611-481e-81ed-a64ad4fa47cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
