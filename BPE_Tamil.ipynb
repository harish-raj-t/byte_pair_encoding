{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e3f5ba2-085b-44e4-9c13-af9d3b08ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text):\n",
    "    return [line.strip()+' <EOS>' for line in text.split(\".\") if len(line.split(\" \")) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "325f0482-956b-41e0-80d3-afbae91e720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoder:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  \n",
    "        self.inverse_vocab = {} \n",
    "        self.merges = []  \n",
    "\n",
    "    def train(self, content):\n",
    "        texts = extract_sentences(content)\n",
    "        #word_counts = {low:1, lower:2, high:1}\n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            idx = 0\n",
    "            for word in text.split():\n",
    "                if idx!=0:\n",
    "                    word = \"Ġ\"+word\n",
    "                else:\n",
    "                    idx += 1\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 0\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        #word_vocab = {l o w:1, l o w e r:2, h i g h:1}\n",
    "        word_vocab = {}\n",
    "\n",
    "        for word, count in word_counts.items():\n",
    "            if word == 'Ġ<EOS>':\n",
    "                word_vocab['Ġ' + ' <EOS>'] = count  \n",
    "        \n",
    "            parts = word.split('Ġ')\n",
    "            core_word = parts[-1]  \n",
    "        \n",
    "            if re.match(r'^[\\d,.-]+$', core_word):  \n",
    "                word_vocab['Ġ ' + core_word] = count  \n",
    "        \n",
    "            elif re.search(r'[க-ஹ]+[a-zA-Z]+|[a-zA-Z]+[க-ஹ]+', core_word):  \n",
    "                word_vocab['Ġ ' + core_word] = count  \n",
    "        \n",
    "            elif re.match(r'^[\\W_]+$', core_word):  \n",
    "                word_vocab['Ġ ' + core_word] = count  \n",
    "        \n",
    "            else:\n",
    "                chars = tamil.utf8.get_letters(word) \n",
    "                word_vocab[\" \".join(chars)] = count \n",
    "\n",
    "        #initial_vocab = {l, o ,w, e, r, h, i, g}\n",
    "        initial_vocab = set()\n",
    "        for word in word_vocab.keys():\n",
    "            initial_vocab.update(word.split())\n",
    "\n",
    "        #'b': 0, # 'd': 1}, self.vocab[char]\n",
    "        #{0: 'b',1: 'd'} self.inverse_vocab[i]\n",
    "        for i, char in enumerate(sorted(initial_vocab)):\n",
    "            self.vocab[char] = i\n",
    "            self.inverse_vocab[i] = char\n",
    "        \n",
    "        #next_id = 12 for example\n",
    "        next_id = len(self.vocab)\n",
    "        print(next_id)\n",
    "        \n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = self._get_pairs(word_vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            print(best_pair)\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            self.vocab[new_token] = next_id\n",
    "            self.inverse_vocab[next_id] = new_token\n",
    "            next_id += 1\n",
    "            \n",
    "            new_vocab = {}\n",
    "            bigram = ' '.join(best_pair)\n",
    "            replacement = ''.join(best_pair)\n",
    "            \n",
    "            for word, count in word_vocab.items():\n",
    "                new_word = word.replace(bigram, replacement)\n",
    "                new_vocab[new_word] = count\n",
    "                \n",
    "            word_vocab = new_vocab\n",
    "            \n",
    "            if len(self.vocab) >= self.vocab_size:\n",
    "                l = len(self.vocab)\n",
    "                self.vocab['<UNK>'] = len(l)\n",
    "                self.inverse_vocab[l] = '<UNK>'\n",
    "                break\n",
    "\n",
    "        \n",
    "    \n",
    "    def _get_pairs(self, word_vocab):\n",
    "        pairs = {}\n",
    "        for word, count in word_vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + count\n",
    "        return pairs\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        words = text.split()\n",
    "        idx = 0\n",
    "        for word in text.split():\n",
    "            if idx!=0:\n",
    "                word_to_encode = \"Ġ\"+word\n",
    "            else:\n",
    "                word_to_encode = word\n",
    "                idx += 1\n",
    "            \n",
    "            if re.match(r'^[\\d,.-]+$', word):  \n",
    "                if i != 0:\n",
    "                    tokens.append(self.vocab.get('Ġ', self.vocab.get(' ')))\n",
    "                tokens.extend([self.vocab.get(char, self._get_unknown_token_id()) for char in word])\n",
    "                continue\n",
    "            elif re.search(r'[க-ஹ]+[a-zA-Z]+|[a-zA-Z]+[க-ஹ]+', word):  # Mixed scripts\n",
    "                if i != 0:\n",
    "                    tokens.append(self.vocab.get('Ġ', self.vocab.get(' ')))\n",
    "                tokens.extend([self.vocab.get(char, self._get_unknown_token_id()) for char in word])\n",
    "                continue\n",
    "            elif re.match(r'^[\\W_]+$', word):  # Non-word characters\n",
    "                if i != 0:\n",
    "                    tokens.append(self.vocab.get('Ġ', self.vocab.get(' ')))\n",
    "                tokens.extend([self.vocab.get(char, self._get_unknown_token_id()) for char in word])\n",
    "                continue\n",
    "            \n",
    "            chars = tamil.utf8.get_letters(word_to_encode)\n",
    "            word_tokens = \" \".join(chars)\n",
    "            \n",
    "            for pair in self.merges:\n",
    "                bigram = ' '.join(pair)\n",
    "                replacement = ''.join(pair)\n",
    "                word_tokens = word_tokens.replace(bigram, replacement)\n",
    "            \n",
    "            for subword in word_tokens.split():\n",
    "                if subword in self.vocab:\n",
    "                    tokens.append(self.vocab[subword])\n",
    "                else:\n",
    "                    for char in subword:\n",
    "                        if char in self.vocab:\n",
    "                            tokens.append(self.vocab[char])\n",
    "                        else:\n",
    "                            tokens.append(self._get_unknown_token_id())\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def _get_unknown_token_id(self):\n",
    "        return self.vocab.get('<UNK>', len(self.vocab))\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        tokens = [self.inverse_vocab.get(id, '<UNK>') for id in ids]\n",
    "        \n",
    "        text = \"\"\n",
    "        for token in tokens:\n",
    "            if token.startswith('Ġ'):\n",
    "                text += ' ' + token[1:]\n",
    "            else:\n",
    "                text += token\n",
    "        \n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6bb88202-04ae-4bdb-b36f-e22c5636a5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "('i', 's')\n",
      "('<', 'E')\n",
      "('<E', 'O')\n",
      "('<EO', 'S')\n",
      "('<EOS', '>')\n",
      "('t', 'h')\n",
      "('th', 'is')\n",
      "('Ġ', 'is')\n",
      "('Ġ', 'இ')\n",
      "('Ġஇ', 'த்')\n",
      "('Ġஇத்', 'தி')\n",
      "('Ġஇத்தி', 'ரை')\n",
      "('Ġஇத்திரை', 'ப்')\n",
      "('Ġஇத்திரைப்', 'ப')\n",
      "('Ġஇத்திரைப்ப', 'ட')\n",
      "('Ġஇத்திரைப்பட', 'ம்')\n",
      "('Ġ', '<EOS>')\n",
      "Vocabulary:\n",
      "0: '<'\n",
      "2: '>'\n",
      "3: 'E'\n",
      "4: 'O'\n",
      "5: 'S'\n",
      "6: 'h'\n",
      "7: 'i'\n",
      "8: 's'\n",
      "9: 't'\n",
      "10: 'Ġ'\n",
      "11: 'இ'\n",
      "12: 'ட'\n",
      "13: 'தி'\n",
      "14: 'த்'\n",
      "15: 'ப'\n",
      "16: 'ப்'\n",
      "17: 'ம்'\n",
      "18: 'ரை'\n",
      "19: 'is'\n",
      "20: '<E'\n",
      "21: '<EO'\n",
      "22: '<EOS'\n",
      "23: '<EOS>'\n",
      "24: 'th'\n",
      "25: 'this'\n",
      "26: 'Ġis'\n",
      "27: 'Ġஇ'\n",
      "28: 'Ġஇத்'\n",
      "29: 'Ġஇத்தி'\n",
      "30: 'Ġஇத்திரை'\n",
      "31: 'Ġஇத்திரைப்'\n",
      "32: 'Ġஇத்திரைப்ப'\n",
      "33: 'Ġஇத்திரைப்பட'\n",
      "34: 'Ġஇத்திரைப்படம்'\n",
      "35: 'Ġ<EOS>'\n",
      "\n",
      "Merges:\n",
      "1: i + s -> is\n",
      "2: < + E -> <E\n",
      "3: <E + O -> <EO\n",
      "4: <EO + S -> <EOS\n",
      "5: <EOS + > -> <EOS>\n",
      "6: t + h -> th\n",
      "7: th + is -> this\n",
      "8: Ġ + is -> Ġis\n",
      "9: Ġ + இ -> Ġஇ\n",
      "10: Ġஇ + த் -> Ġஇத்\n",
      "11: Ġஇத் + தி -> Ġஇத்தி\n",
      "12: Ġஇத்தி + ரை -> Ġஇத்திரை\n",
      "13: Ġஇத்திரை + ப் -> Ġஇத்திரைப்\n",
      "14: Ġஇத்திரைப் + ப -> Ġஇத்திரைப்ப\n",
      "15: Ġஇத்திரைப்ப + ட -> Ġஇத்திரைப்பட\n",
      "16: Ġஇத்திரைப்பட + ம் -> Ġஇத்திரைப்படம்\n",
      "17: Ġ + <EOS> -> Ġ<EOS>\n",
      "\n",
      "Original: என்ன பண்ணனும்\n",
      "Encoded: [15]\n",
      "Decoded: ப\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample training data\n",
    "    content = '''\n",
    "    this is a test. இத்திரைப்படம் 22 பெப்ரவரி 2019 அன்று திரைக்கு வந்தது. இத்திரைப்படத்தை லியோன் ஜேம்ஸ் இயக்கியுள்ளார். இத்திரைப்படத்தை வேல் பிலிம்ஸ் இன்டர்நேஷனல் தயாரித்துள்ளனர். இத்திரைப்படத்தில் நாஞ்சில் சம்பத், ஆர். கே. ரித்திஷ், மயில்சாமி ஆகியோரும் நடித்துள்ளனர். லால்குடி கருப்பையா காந்தி என்பதன் சுருக்கமே எல். கே. ஜி என்பதாகக் கூறப்படுகிறது. திரைப்படம் முழுவதும் சமகால அரசியல் சூழலை பகடி செய்யும் விதத்தில் அமைந்துள்ளதாகவும், அதே நேரத்தில் அரசியல் விழிப்புணர்வை ஏற்படுத்தும் நகைச்சுவைத் திரைப்படமாகவும் வந்துள்ளதாக விமர்சனங்கள் தெரிவிக்கின்றன.\n",
    "    '''\n",
    "    content = '''\n",
    "    this is இத்திரைப்படம். \n",
    "    '''\n",
    "    # texts = extract_sentences(content)\n",
    "    # print(texts)\n",
    "    \n",
    "    # texts = [\n",
    "    #     \"low lower lowest\",\n",
    "    #     \"newer new newest lower\",\n",
    "    #     \"wider width widest\",\n",
    "    #     \"better best\"\n",
    "    # ]\n",
    "    \n",
    "    # Create and train the BPE model\n",
    "    bpe = BytePairEncoder(vocab_size=120)\n",
    "    bpe.train(content)\n",
    "    \n",
    "    # Print the vocabulary and merges\n",
    "    print(\"Vocabulary:\")\n",
    "    for token, id in sorted(bpe.vocab.items(), key=lambda x: x[1]):\n",
    "        print(f\"{id}: '{token}'\")\n",
    "    \n",
    "    print(\"\\nMerges:\")\n",
    "    for i, merge in enumerate(bpe.merges):\n",
    "        print(f\"{i+1}: {merge[0]} + {merge[1]} -> {merge[0] + merge[1]}\")\n",
    "    \n",
    "    # Test encoding and decoding\n",
    "    test_text = \"என்ன பண்ணனும்\"\n",
    "    encoded = bpe.encode(test_text)\n",
    "    decoded = bpe.decode(encoded)\n",
    "    \n",
    "    print(f\"\\nOriginal: {test_text}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "\n",
    "    # word_counts = {}\n",
    "    # for text in texts:\n",
    "    #     idx = 0\n",
    "    #     for word in text.split():\n",
    "    #             if idx!=0:\n",
    "    #                 word = \"Ġ\"+word\n",
    "    #             else:\n",
    "    #                 idx += 1\n",
    "    #             if word not in word_counts:\n",
    "    #                 word_counts[word] = 0\n",
    "    #             word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6451b72-638a-40e5-a3d2-0001b021c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<': 0,\n",
       " '<EOS>': 1,\n",
       " '>': 2,\n",
       " 'E': 3,\n",
       " 'O': 4,\n",
       " 'S': 5,\n",
       " 'h': 6,\n",
       " 'i': 7,\n",
       " 's': 8,\n",
       " 't': 9,\n",
       " 'Ġ': 10,\n",
       " 'இ': 11,\n",
       " 'ட': 12,\n",
       " 'தி': 13,\n",
       " 'த்': 14,\n",
       " 'ப': 15,\n",
       " 'ப்': 16,\n",
       " 'ம்': 17,\n",
       " 'ரை': 18,\n",
       " 'is': 996985}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "218e6977-5f01-4af6-9d8e-17a81a7be7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tamil\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if word == 'Ġ<EOS>':\n",
    "        vocab['Ġ' + ' <EOS>'] = count  \n",
    "\n",
    "    parts = word.split('Ġ')\n",
    "    core_word = parts[-1]  \n",
    "\n",
    "    if re.match(r'^[\\d,.-]+$', core_word):  \n",
    "        vocab['Ġ ' + core_word] = count  \n",
    "\n",
    "    elif re.search(r'[க-ஹ]+[a-zA-Z]+|[a-zA-Z]+[க-ஹ]+', core_word):  \n",
    "        vocab['Ġ ' + core_word] = count  \n",
    "\n",
    "    elif re.match(r'^[\\W_]+$', core_word):  \n",
    "        vocab['Ġ ' + core_word] = count  \n",
    "\n",
    "    else:\n",
    "        chars = tamil.utf8.get_letters(word) \n",
    "        vocab[\" \".join(chars)] = count  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec2bfd09-c5b4-4e8c-8dbb-e24e490a3548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t h i s': 1,\n",
       " 'Ġ i s': 1,\n",
       " 'Ġ a': 1,\n",
       " 'Ġ t e s t': 1,\n",
       " 'Ġ <EOS>': 9,\n",
       " 'Ġ < E O S >': 9,\n",
       " 'இ த் தி ரை ப் ப ட ம்': 1,\n",
       " 'Ġ 22': 1,\n",
       " 'Ġ பெ ப் ர வ ரி': 1,\n",
       " 'Ġ 2019': 1,\n",
       " 'Ġ அ ன் று': 1,\n",
       " 'Ġ தி ரை க் கு': 1,\n",
       " 'Ġ வ ந் த து': 1,\n",
       " 'இ த் தி ரை ப் ப ட த் தை': 2,\n",
       " 'Ġ லி யோ ன்': 1,\n",
       " 'Ġ ஜே ம் ஸ்': 1,\n",
       " 'Ġ இ ய க் கி யு ள் ளா ர்': 1,\n",
       " 'Ġ வே ல்': 1,\n",
       " 'Ġ பி லி ம் ஸ்': 1,\n",
       " 'Ġ இ ன் ட ர் நே ஷ ன ல்': 1,\n",
       " 'Ġ த யா ரி த் து ள் ள ன ர்': 1,\n",
       " 'இ த் தி ரை ப் ப ட த் தி ல்': 1,\n",
       " 'Ġ நா ஞ் சி ல்': 1,\n",
       " 'Ġ ச ம் ப த் ,': 1,\n",
       " 'Ġ ஆ ர்': 1,\n",
       " 'ரி த் தி ஷ் ,': 1,\n",
       " 'Ġ ம யி ல் சா மி': 1,\n",
       " 'Ġ ஆ கி யோ ரு ம்': 1,\n",
       " 'Ġ ந டி த் து ள் ள ன ர்': 1,\n",
       " 'லா ல் கு டி': 1,\n",
       " 'Ġ க ரு ப் பை யா': 1,\n",
       " 'Ġ கா ந் தி': 1,\n",
       " 'Ġ எ ன் ப த ன்': 1,\n",
       " 'Ġ சு ரு க் க மே': 1,\n",
       " 'Ġ எ ல்': 1,\n",
       " 'ஜி': 1,\n",
       " 'Ġ எ ன் ப தா க க்': 1,\n",
       " 'Ġ கூ ற ப் ப டு கி ற து': 1,\n",
       " 'தி ரை ப் ப ட ம்': 1,\n",
       " 'Ġ மு ழு வ து ம்': 1,\n",
       " 'Ġ ச ம கா ல': 1,\n",
       " 'Ġ அ ர சி ய ல்': 2,\n",
       " 'Ġ சூ ழ லை': 1,\n",
       " 'Ġ ப க டி': 1,\n",
       " 'Ġ செ ய் யு ம்': 1,\n",
       " 'Ġ வி த த் தி ல்': 1,\n",
       " 'Ġ அ மை ந் து ள் ள தா க வு ம் ,': 1,\n",
       " 'Ġ அ தே': 1,\n",
       " 'Ġ நே ர த் தி ல்': 1,\n",
       " 'Ġ வி ழி ப் பு ண ர் வை': 1,\n",
       " 'Ġ ஏ ற் ப டு த் து ம்': 1,\n",
       " 'Ġ ந கை ச் சு வை த்': 1,\n",
       " 'Ġ தி ரை ப் ப ட மா க வு ம்': 1,\n",
       " 'Ġ வ ந் து ள் ள தா க': 1,\n",
       " 'Ġ வி ம ர் ச ன ங் க ள்': 1,\n",
       " 'Ġ தெ ரி வி க் கி ன் ற ன': 1,\n",
       " '< E O S >': 1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "004a7d82-d43a-4218-b6fc-0f2d4f69665a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['த', 'மி', 'ழ்']\n"
     ]
    }
   ],
   "source": [
    "import tamil\n",
    "\n",
    "word = \"தமிழ்\"\n",
    "syllables = tamil.utf8.get_letters(word)\n",
    "print(syllables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575ae6f5-b202-4edf-b831-b7695b7dd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocab = set()\n",
    "for word in vocab.keys():\n",
    "    initial_vocab.update(word.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3624329-b8d3-4f00-8196-71fefe5245e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'b': 0,\n",
       "  'd': 1,\n",
       "  'e': 2,\n",
       "  'h': 3,\n",
       "  'i': 4,\n",
       "  'l': 5,\n",
       "  'n': 6,\n",
       "  'o': 7,\n",
       "  'r': 8,\n",
       "  's': 9,\n",
       "  't': 10,\n",
       "  'w': 11},\n",
       " {0: 'b',\n",
       "  1: 'd',\n",
       "  2: 'e',\n",
       "  3: 'h',\n",
       "  4: 'i',\n",
       "  5: 'l',\n",
       "  6: 'n',\n",
       "  7: 'o',\n",
       "  8: 'r',\n",
       "  9: 's',\n",
       "  10: 't',\n",
       "  11: 'w'},\n",
       " 12,\n",
       " {'b', 'd', 'e', 'h', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, inverse_vocab, next_id, initial_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e7a20-ae32-4f1c-bd8c-3d483274e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20\n",
    "while len(vocab) < self.vocab_size:\n",
    "            pairs = self._get_pairs(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Add the merge to our list\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # Update vocabulary with the new token\n",
    "            self.vocab[new_token] = next_id\n",
    "            self.inverse_vocab[next_id] = new_token\n",
    "            next_id += 1\n",
    "            \n",
    "            # Update the word counts with the new merged pair\n",
    "            new_vocab = {}\n",
    "            bigram = ' '.join(best_pair)\n",
    "            replacement = ''.join(best_pair)\n",
    "            \n",
    "            for word, count in vocab.items():\n",
    "                new_word = word.replace(bigram, replacement)\n",
    "                new_vocab[new_word] = count\n",
    "                \n",
    "            vocab = new_vocab\n",
    "            \n",
    "            # Stop if we've reached the desired vocab size\n",
    "            if len(self.vocab) >= self.vocab_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccf904e-7c38-4e5c-af38-05f69e71ef3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['வ', 'ண', 'க', '்', 'க', 'ம', '்']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"வணக்கம்\"\n",
    "[i for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9065fb-16e7-4299-ad59-0f178f248569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pairs(word_vocab):\n",
    "        pairs = {}\n",
    "        for word, count in word_vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + count\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66627000-cb2c-46c5-ad52-4752716963c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o'): 1, ('o', 'w'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_pairs({'l o w': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc24f0-c61f-4cb2-a761-fef3ef2994ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
